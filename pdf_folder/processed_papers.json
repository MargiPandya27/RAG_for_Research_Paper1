{
    "Paper_1": {
        "title": "Image Segmentation",
        "sections": [
            {
                "section": "Abstract",
                "content": ". There is large consent that successful training of deep net- works requires many thousand annotated training samples. In this pa- per, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more eﬃciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localiza- tion. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neu- ronal structures in electron microscopic stacks. Using the same net- work trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these cate- gories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caﬀe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net. 1"
            },
            {
                "section": "Introduction",
                "content": "In the last two years, deep convolutional networks have outperformed the state of the art in many visual recognition tasks, e.g. [7,3]. While convolutional networks have already existed for a long time [8], their success was limited due to the size of the available training sets and the size of the considered networks. The breakthrough by Krizhevsky et al. [7] was due to supervised training of a large network with 8 layers and millions of parameters on the ImageNet dataset with 1 million training images. Since then, even larger and deeper networks have been trained [12]. The typical use of convolutional networks is on classiﬁcation tasks, where the output to an image is a single class label. However, in many visual tasks, especially in biomedical image processing, the desired output should include localization, i.e., a class label is supposed to be assigned to each pixel. More- over, thousands of training images are usually beyond reach in biomedical tasks. Hence, Ciresan et al. [1] trained a network in a sliding-window setup to predict the class label of each pixel by providing a local region (patch) around that pixel arXiv:1505.04597v1  [cs.CV]  18 May 2015 2 copy and crop input image tile output segmentation map 64 1 128 256 512 1024 max pool 2x2 up-conv 2x2 conv 3x3, ReLU 572 x 572 284² 64 128 256 512 570 x 570 568 x 568 282² 280² 140² 138² 136² 68² 66² 64² 32² 28² 56² 54² 52² 512 104² 102² 100² 200² 30² 198² 196² 392 x 392 390 x 390 388 x 388 388 x 388 1024 512 256 256 128 64 128 64 2 conv 1x1 Fig. 1. U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the diﬀerent operations. as input. First, this network can localize. Secondly, the training data in terms of patches is much larger than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a large margin. Obviously, the strategy in Ciresan et al. [1] has two drawbacks. First, it is quite slow because the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-oﬀbetween localization accuracy and the use of context. Larger patches require more max-pooling layers that reduce the localization accuracy, while small patches allow the network to see only little context. More recent approaches [11,4] proposed a classiﬁer output that takes into account the features from multiple layers. Good localization and the use of context are possible at the same time. In this paper, we build upon a more elegant architecture, the so-called “fully convolutional network” [9]. We modify and extend this architecture such that it works with very few training images and yields more precise segmentations; see Figure 1. The main idea in [9] is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators. Hence, these layers increase the resolution of the output. In order to localize, high resolution features from the contracting path are combined with the upsampled 3 Fig. 2. Overlap-tile strategy for seamless segmentation of arbitrary large images (here segmentation of neuronal structures in EM stacks). Prediction of the segmentation in the yellow area, requires image data within the blue area as input. Missing input data is extrapolated by mirroring output. A successive convolution layer can then learn to assemble a more precise output based on this information. One important modiﬁcation in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. This strategy allows the seamless segmentation of arbitrarily large images by an overlap-tile strategy (see Figure 2). To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory. As for our tasks there is very little training data available, we use excessive data augmentation by applying elastic deformations to the available training im- ages. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. This is particularly important in biomedical segmentation, since deformation used to be the most common variation in tissue and realistic deformations can be simu- lated eﬃciently. The value of data augmentation for learning invariance has been shown in Dosovitskiy et al. [2] in the scope of unsupervised feature learning. Another challenge in many cell segmentation tasks is the separation of touch- ing objects of the same class; see Figure 3. To this end, we propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function. The resulting network is applicable to various biomedical segmentation prob- lems. In this paper, we show results on the segmentation of neuronal structures in EM stacks (an ongoing competition started at ISBI 2012), where we out- 4 performed the network of Ciresan et al. [1]. Furthermore, we show results for cell segmentation in light microscopy images from the ISBI cell tracking chal- lenge 2015. Here we won with a large margin on the two most challenging 2D transmitted light datasets. 2 Network Architecture The network architecture is illustrated in Figure 1. It consists of a contracting path (left side) and an expansive path (right side). The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectiﬁed linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each fol- lowed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the ﬁnal layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes. In total the network has 23 convolutional layers. To allow a seamless tiling of the output segmentation map (see Figure 2), it is important to select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size. 3 Training The input images and their corresponding segmentation maps are used to train the network with the stochastic gradient descent implementation of Caﬀe [6]. Due to the unpadded convolutions, the output image is smaller than the input by a constant border width. To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image. Accordingly we use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step. The energy function is computed by a pixel-wise soft-max over the ﬁnal feature map combined with the cross entropy loss function. The soft-max is deﬁned as pk(x) = exp(ak(x))/ \u0010PK k′=1 exp(ak′(x)) \u0011 where ak(x) denotes the activation in feature channel k at the pixel position x ∈Ωwith Ω⊂Z2. K is the number of classes and pk(x) is the approximated maximum-function. I.e. pk(x) ≈1 for the k that has the maximum activation ak(x) and pk(x) ≈0 for all other k. The cross entropy then penalizes at each position the deviation of pℓ(x)(x) from 1 using E = X x∈Ω w(x) log(pℓ(x)(x)) (1) 5 a b c d Fig. 3. HeLa cells on glass recorded with DIC (diﬀerential interference contrast) mi- croscopy. (a) raw image. (b) overlay with ground truth segmentation. Diﬀerent colors indicate diﬀerent instances of the HeLa cells. (c) generated segmentation mask (white: foreground, black: background). (d) map with a pixel-wise loss weight to force the network to learn the border pixels. where ℓ: Ω→{1, . . . , K} is the true label of each pixel and w : Ω→R is a weight map that we introduced to give some pixels more importance in the training. We pre-compute the weight map for each ground truth segmentation to com- pensate the diﬀerent frequency of pixels from a certain class in the training data set, and to force the network to learn the small separation borders that we introduce between touching cells (See Figure 3c and d). The separation border is computed using morphological operations. The weight map is then computed as w(x) = wc(x) + w0 · exp −(d1(x) + d2(x))2 2σ2 ! (2) where wc : Ω→R is the weight map to balance the class frequencies, d1 : Ω→R denotes the distance to the border of the nearest cell and d2 : Ω→R the distance to the border of the second nearest cell. In our experiments we set w0 = 10 and σ ≈5 pixels. In deep networks with many convolutional layers and diﬀerent paths through the network, a good initialization of the weights is extremely important. Oth- erwise, parts of the network might give excessive activations, while other parts never contribute. Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. For a network with our architecture (alternating convolution and ReLU layers) this can be achieved by drawing the initial weights from a Gaussian distribution with a standard deviation of p 2/N, where N denotes the number of incoming nodes of one neu- ron [5]. E.g. for a 3x3 convolution and 64 feature channels in the previous layer N = 9 · 64 = 576. 3.1 Data Augmentation Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of 6 microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elas- tic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images. We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpola- tion. Drop-out layers at the end of the contracting path perform further implicit data augmentation. 4 Experiments We demonstrate the application of the u-net to three diﬀerent segmentation tasks. The ﬁrst task is the segmentation of neuronal structures in electron mi- croscopic recordings. An example of the data set and our obtained segmentation is displayed in Figure 2. We provide the full result as Supplementary Material. The data set is provided by the EM segmentation challenge [14] that was started at ISBI 2012 and is still open for new contributions. The training data is a set of 30 images (512x512 pixels) from serial section transmission electron microscopy of the Drosophila ﬁrst instar larva ventral nerve cord (VNC). Each image comes with a corresponding fully annotated ground truth segmentation map for cells (white) and membranes (black). The test set is publicly available, but its seg- mentation maps are kept secret. An evaluation can be obtained by sending the predicted membrane probability map to the organizers. The evaluation is done by thresholding the map at 10 diﬀerent levels and computation of the “warping error”, the “Rand error” and the “pixel error” [14]. The u-net (averaged over 7 rotated versions of the input data) achieves with- out any further pre- or postprocessing a warping error of 0.0003529 (the new best score, see Table 1) and a rand-error of 0.0382. This is signiﬁcantly better than the sliding-window convolutional network result by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing Table 1. Ranking on the EM segmentation challenge [14] (march 6th, 2015), sorted by warping error. Rank Group name Warping Error Rand Error Pixel Error ** human values ** 0.000005 0.0021 0.0010 1. u-net 0.000353 0.0382 0.0611 2. DIVE-SCI 0.000355 0.0305 0.0584 3. IDSIA [1] 0.000420 0.0504 0.0613 4. DIVE 0.000430 0.0545 0.0582 ... 10. IDSIA-SCI 0.000653 0.0189 0.1027 7 a b c d Fig. 4. Result on the ISBI cell tracking challenge. (a) part of an input image of the “PhC-U373” data set. (b) Segmentation result (cyan mask) with manual ground truth (yellow border) (c) input image of the “DIC-HeLa” data set. (d) Segmentation result (random colored masks) with manual ground truth (yellow border). Table 2. Segmentation results (IOU) on the ISBI cell tracking challenge 2015. Name PhC-U373 DIC-HeLa IMCB-SG (2014) 0.2669 0.2935 KTH-SE (2014) 0.7953 0.4607 HOUS-US (2014) 0.5323 - second-best 2015 0.83 0.46 u-net (2015) 0.9203 0.7756 algorithms on this data set use highly data set speciﬁc post-processing methods1 applied to the probability map of Ciresan et al. [1]. We also applied the u-net to a cell segmentation task in light microscopic im- ages. This segmenation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13]. The ﬁrst data set “PhC-U373”2 contains Glioblastoma-astrocytoma U373 cells on a polyacrylimide substrate recorded by phase contrast microscopy (see Figure 4a,b and Supp. Material). It contains 35 partially annotated train- ing images. Here we achieve an average IOU (“intersection over union”) of 92%, which is signiﬁcantly better than the second best algorithm with 83% (see Ta- ble 2). The second data set “DIC-HeLa”3 are HeLa cells on a ﬂat glass recorded by diﬀerential interference contrast (DIC) microscopy (see Figure 3, Figure 4c,d and Supp. Material). It contains 20 partially annotated training images. Here we achieve an average IOU of 77.5% which is signiﬁcantly better than the second best algorithm with 46%. 5"
            },
            {
                "section": "Conclusion",
                "content": "The u-net architecture achieves very good performance on very diﬀerent biomed- ical segmentation applications. Thanks to data augmentation with elastic defor- 1 The authors of this algorithm have submitted 78 diﬀerent solutions to achieve this result. 2 Data set provided by Dr. Sanjay Kumar. Department of Bioengineering University of California at Berkeley. Berkeley CA (USA) 3 Data set provided by Dr. Gert van Cappellen Erasmus Medical Center. Rotterdam. The Netherlands 8 mations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVidia Titan GPU (6 GB). We provide the full Caﬀe[6]-based implementation and the trained networks4. We are sure that the u-net architecture can be applied easily to many more tasks. Acknowlegements This study was supported by the Excellence Initiative of the German Federal and State governments (EXC 294) and by the BMBF (Fkz 0316185B)."
            }
        ]
    },
    "Paper_2": {
        "title": "Pyramid Scene Parsing Network",
        "sections": [
            {
                "section": "Abstract",
                "content": "Scene parsing is challenging for unrestricted open vo- cabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region- based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is ef- fective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel- level prediction. The proposed approach achieves state-of- the-art performance on various datasets. It came ﬁrst in Im- ageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
            },
            {
                "section": "Introduction",
                "content": "Scene parsing, based on semantic segmentation, is a fun- damental topic in computer vision. The goal is to assign each pixel in the image a category label. Scene parsing pro- vides complete understanding of the scene. It predicts the label, location, as well as shape for each element. This topic is of broad interest for potential applications of automatic driving, robot sensing, to name a few. Difﬁculty of scene parsing is closely related to scene and label variety. The pioneer scene parsing task [23] is to clas- sify 33 scenes for 2,688 images on LMO dataset [22]. More recent PASCAL VOC semantic segmentation and PASCAL context datasets [8, 29] include more labels with similar context, such as chair and sofa, horse and cow, etc. The new ADE20K dataset [43] is the most challenging one with a large and unrestricted open vocabulary and more scene classes. A few representative images are shown in Fig. 1. To develop an effective algorithm for these datasets needs to conquer a few difﬁculties. State-of-the-art scene parsing frameworks are mostly based on the fully convolutional network (FCN) [26]. The deep convolutional neural network (CNN) based methods boost dynamic object understanding, and yet still face chal- Figure 1. Illustration of complex scenes in ADE20K dataset. lenges considering diverse scenes and unrestricted vocabu- lary. One example is shown in the ﬁrst row of Fig. 2, where a boat is mistaken as a car. These errors are due to similar appearance of objects. But when viewing the image regard- ing the context prior that the scene is described as boathouse near a river, correct prediction should be yielded. Towards accurate scene perception, the knowledge graph relies on prior information of scene context. We found that the major issue for current FCN based models is lack of suitable strategy to utilize global scene category clues. For typical complex scene understanding, previously to get a global image-level feature, spatial pyramid pooling [18] was widely employed where spatial statistics provide a good descriptor for overall scene interpretation. Spatial pyramid pooling network [12] further enhances the ability. Different from these methods, to incorporate suitable global features, we propose pyramid scene parsing network (PSPNet). In addition to traditional dilated FCN [3, 40] for pixel prediction, we extend the pixel-level feature to the specially designed global pyramid pooling one. The local and global clues together make the ﬁnal prediction more reliable. We also propose an optimization strategy with 1 arXiv:1612.01105v2  [cs.CV]  27 Apr 2017 deeply supervised loss. We give all implementation details, which are key to our decent performance in this paper, and make the code and trained models publicly available 1. Our approach achieves state-of-the-art performance on all available datasets. It is the champion of ImageNet scene parsing challenge 2016 [43], and arrived the 1st place on PASCAL VOC 2012 semantic segmentation benchmark [8], and the 1st place on urban scene Cityscapes data [6]. They manifest that PSPNet gives a promising direction for pixel- level prediction tasks, which may even beneﬁt CNN-based stereo matching, optical ﬂow, depth estimation, etc. in follow-up work. Our main contributions are threefold. • We propose a pyramid scene parsing network to em- bed difﬁcult scenery context features in an FCN based pixel prediction framework. • We develop an effective optimization strategy for deep ResNet [13] based on deeply supervised loss. • We build a practical system for state-of-the-art scene parsing and semantic segmentation where all crucial implementation details are included. 2. Related Work In the following, we review recent advances in scene parsing and semantic segmentation tasks. Driven by pow- erful deep neural networks [17, 33, 34, 13], pixel-level prediction tasks like scene parsing and semantic segmen- tation achieve great progress inspired by replacing the fully-connected layer in classiﬁcation with the convolution layer [26]. To enlarge the receptive ﬁeld of neural networks,"
            },
            {
                "section": "methods",
                "content": "of [3, 40] used dilated convolution. Noh et al. [30] proposed a coarse-to-ﬁne structure with deconvolution net- work to learn the segmentation mask. Our baseline network is FCN and dilated network [26, 3]. Other work mainly proceeds in two directions. One line [26, 3, 5, 39, 11] is with multi-scale feature ensembling. Since in deep networks, higher-layer feature contains more semantic meaning and less location information. Combin- ing multi-scale features can improve the performance. The other direction is based on structure prediction. The pioneer work [3] used conditional random ﬁeld (CRF) as post processing to reﬁne the segmentation result. Following"
            },
            {
                "section": "methods",
                "content": "[25, 41, 1] reﬁned networks via end-to-end model- ing. Both of the two directions ameliorate the localization ability of scene parsing where predicted semantic boundary ﬁts objects. Yet there is still much room to exploit necessary information in complex scenes. To make good use of global image-level priors for di- verse scene understanding, methods of [18, 27] extracted global context information with traditional features not from deep neural networks. Similar improvement was made 1https://github.com/hszhao/PSPNet under object detection frameworks [35]. Liu et al. [24] proved that global average pooling with FCN can improve semantic segmentation results. However, our experiments show that these global descriptors are not representative enough for the challenging ADE20K data. Therefore, dif- ferent from global pooling in [24], we exploit the capabil- ity of global context information by different-region-based context aggregation via our pyramid scene parsing network. 3. Pyramid Scene Parsing Network We start with our observation and analysis of represen- tative failure cases when applying FCN methods to scene parsing. They motivate proposal of our pyramid pooling module as the effective global context prior. Our pyramid scene parsing network (PSPNet) illustrated in Fig. 3 is then described to improve performance for open-vocabulary ob- ject and stuff identiﬁcation in complex scene parsing. 3.1. Important Observations The new ADE20K dataset [43] contains 150 stuff/object category labels (e.g., wall, sky, and tree) and 1,038 image- level scene descriptors (e.g., airport terminal, bedroom, and street). So a large amount of labels and vast distributions of scenes come into existence. Inspecting the prediction"
            },
            {
                "section": "results",
                "content": "of the FCN baseline provided in [43], we summarize several common issues for complex-scene parsing. Mismatched Relationship Context relationship is uni- versal and important especially for complex scene under- standing. There exist co-occurrent visual patterns. For ex- ample, an airplane is likely to be in runway or ﬂy in sky while not over a road. For the ﬁrst-row example in Fig. 2, FCN predicts the boat in the yellow box as a “car” based on its appearance. But the common knowledge is that a car is seldom over a river. Lack of the ability to collect contextual information increases the chance of misclassiﬁcation. Confusion Categories There are many class label pairs in the ADE20K dataset [43] that are confusing in classiﬁ- cation. Examples are ﬁeld and earth; mountain and hill; wall, house, building and skyscraper. They are with simi- lar appearance. The expert annotator who labeled the entire dataset, still makes 17.60% pixel error as described in [43]. In the second row of Fig. 2, FCN predicts the object in the box as part of skyscraper and part of building. These re- sults should be excluded so that the whole object is either skyscraper or building, but not both. This problem can be remedied by utilizing the relationship between categories. Inconspicuous Classes Scene contains objects/stuff of arbitrary size. Several small-size things, like streetlight and signboard, are hard to ﬁnd while they may be of great im- portance. Contrarily, big objects or stuff may exceed the Figure 2. Scene parsing issues we observe on ADE20K [43] dataset. The ﬁrst row shows the issue of mismatched relationship – cars are seldom over water than boats. The second row shows confusion categories where class “building” is easily confused as “skyscraper”. The third row illustrates inconspicuous classes. In this example, the pillow is very similar to the bed sheet in terms of color and texture. These inconspicuous objects are easily misclassiﬁed by FCN. receptive ﬁeld of FCN and thus cause discontinuous pre- diction. As shown in the third row of Fig. 2, the pillow has similar appearance with the sheet. Overlooking the global scene category may fail to parse the pillow. To im- prove performance for remarkably small or large objects, one should pay much attention to different sub-regions that contain inconspicuous-category stuff. To summarize these observations, many errors are par- tially or completely related to contextual relationship and global information for different receptive ﬁelds. Thus a deep network with a suitable global-scene-level prior can much improve the performance of scene parsing. 3.2. Pyramid Pooling Module With above analysis, in what follows, we introduce the pyramid pooling module, which empirically proves to be an effective global contextual prior. In a deep neural network, the size of receptive ﬁeld can roughly indicates how much we use context information. Although theoretically the receptive ﬁeld of ResNet [13] is already larger than the input image, it is shown by Zhou et al. [42] that the empirical receptive ﬁeld of CNN is much smaller than the theoretical one especially on high-level lay- ers. This makes many networks not sufﬁciently incorporate the momentous global scenery prior. We address this issue by proposing an effective global prior representation. Global average pooling is a good baseline model as the global contextual prior, which is commonly used in image classiﬁcation tasks [34, 13]. In [24], it was successfully ap- plied to semantic segmentation. But regarding the complex- scene images in ADE20K [43], this strategy is not enough to cover necessary information. Pixels in these scene images are annotated regarding many stuff and objects. Directly fusing them to form a single vector may lose the spatial rela- tion and cause ambiguity. Global context information along with sub-region context is helpful in this regard to distin- guish among various categories. A more powerful represen- tation could be fused information from different sub-regions with these receptive ﬁelds. Similar conclusion was drawn in classical work [18, 12] of scene/image classiﬁcation. In [12], feature maps in different levels generated by pyramid pooling were ﬁnally ﬂattened and concatenated to be fed into a fully connected layer for classiﬁcation. This global prior is designed to remove the ﬁxed-size constraint of CNN for image classiﬁcation. To further reduce context information loss between different sub-regions, we propose a hierarchical global prior, containing information with dif- ferent scales and varying among different sub-regions. We Figure 3. Overview of our proposed PSPNet. Given an input image (a), we ﬁrst use CNN to get the feature map of the last convolutional layer (b), then a pyramid parsing module is applied to harvest different sub-region representations, followed by upsampling and concatena- tion layers to form the ﬁnal feature representation, which carries both local and global context information in (c). Finally, the representation is fed into a convolution layer to get the ﬁnal per-pixel prediction (d). call it pyramid pooling module for global scene prior con- struction upon the ﬁnal-layer-feature-map of the deep neu- ral network, as illustrated in part (c) of Fig. 3. The pyramid pooling module fuses features under four different pyramid scales. The coarsest level highlighted in red is global pooling to generate a single bin output. The following pyramid level separates the feature map into dif- ferent sub-regions and forms pooled representation for dif- ferent locations. The output of different levels in the pyra- mid pooling module contains the feature map with varied sizes. To maintain the weight of global feature, we use 1×1 convolution layer after each pyramid level to reduce the di- mension of context representation to 1/N of the original one if the level size of pyramid is N. Then we directly up- sample the low-dimension feature maps to get the same size feature as the original feature map via bilinear interpolation. Finally, different levels of features are concatenated as the ﬁnal pyramid pooling global feature. Noted that the number of pyramid levels and size of each level can be modiﬁed. They are related to the size of feature map that is fed into the pyramid pooling layer. The struc- ture abstracts different sub-regions by adopting varying-size pooling kernels in a few strides. Thus the multi-stage ker- nels should maintain a reasonable gap in representation. Our pyramid pooling module is a four-level one with bin sizes of 1×1, 2×2, 3×3 and 6×6 respectively. For the type of pooling operation between max and average, we perform extensive experiments to show the difference in Section 5.2. 3.3. Network Architecture With the pyramid pooling module, we propose our pyra- mid scene parsing network (PSPNet) as illustrated in Fig. 3. Given an input image in Fig. 3(a), we use a pretrained ResNet [13] model with the dilated network strategy [3, 40] to extract the feature map. The ﬁnal feature map size is 1/8 of the input image, as shown in Fig. 3(b). On top of the Figure 4. Illustration of auxiliary loss in ResNet101. Each blue box denotes a residue block. The auxiliary loss is added after the res4b22 residue block. map, we use the pyramid pooling module shown in (c) to gather context information. Using our 4-level pyramid, the pooling kernels cover the whole, half of, and small portions of the image. They are fused as the global prior. Then we concatenate the prior with the original feature map in the ﬁnal part of (c). It is followed by a convolution layer to generate the ﬁnal prediction map in (d). To explain our structure, PSPNet provides an effective global contextual prior for pixel-level scene parsing. The pyramid pooling module can collect levels of information, more representative than global pooling [24]. In terms of computational cost, our PSPNet does not much increase it compared to the original dilated FCN network. In end-to- end learning, the global pyramid pooling module and the local FCN feature can be optimized simultaneously. 4. Deep Supervision for ResNet-Based FCN Deep pretrained networks lead to good performance [17, 33, 13]. However, increasing depth of the network may introduce additional optimization difﬁculty as shown in [32, 19] for image classiﬁcation. ResNet solves this prob- lem with skip connection in each block. Latter layers of deep ResNet mainly learn residues based on previous ones. We contrarily propose generating initial results by super- vision with an additional loss, and learning the residue af- terwards with the ﬁnal loss. Thus, optimization of the deep network is decomposed into two, each is simpler to solve. An example of our deeply supervised ResNet101 [13] model is illustrated in Fig. 4. Apart from the main branch using softmax loss to train the ﬁnal classiﬁer, another clas- siﬁer is applied after the fourth stage, i.e., the res4b22 residue block. Different from relay backpropagation [32] that blocks the backward auxiliary loss to several shallow layers, we let the two loss functions pass through all pre- vious layers. The auxiliary loss helps optimize the learning process, while the master branch loss takes the most respon- sibility. We add weight to balance the auxiliary loss. In the testing phase, we abandon this auxiliary branch and only use the well optimized master branch for ﬁnal pre- diction. This kind of deeply supervised training strategy for ResNet-based FCN is broadly useful under different ex- perimental settings and works with the pre-trained ResNet model. This manifests the generality of such a learning strategy. More details are provided in Section 5.2. 5. Experiments Our proposed method is successful on scene parsing and semantic segmentation challenges. We evaluate it in this section on three different datasets, including ImageNet scene parsing challenge 2016 [43], PASCAL VOC 2012 semantic segmentation [8] and urban scene understanding dataset Cityscapes [6]. 5.1. Implementation Details For a practical deep learning system, devil is always in the details. Our implementation is based on the public plat- form Caffe [15]. Inspired by [4], we use the “poly” learning rate policy where current learning rate equals to the base one multiplying (1 − iter maxiter)power. We set base learning rate to 0.01 and power to 0.9. The performance can be improved by increasing the iteration number, which is set to 150K for ImageNet experiment, 30K for PASCAL VOC and 90K for Cityscapes. Momentum and weight decay are set to 0.9 and 0.0001 respectively. For data augmentation, we adopt ran- dom mirror and random resize between 0.5 and 2 for all datasets, and additionally add random rotation between - 10 and 10 degrees, and random Gaussian blur for ImageNet and PASCAL VOC. This comprehensive data augmentation scheme makes the network resist overﬁtting. Our network contains dilated convolution following [4]. During the course of experiments, we notice that an ap- propriately large “cropsize” can yield good performance and “batchsize” in the batch normalization [14] layer is of great importance. Due to limited physical memory on GPU cards, we set the “batchsize” to 16 during training. To achieve this, we modify Caffe from [37] together with Method Mean IoU(%) Pixel Acc.(%) ResNet50-Baseline 37.23 78.01 ResNet50+B1+MAX 39.94 79.46 ResNet50+B1+AVE 40.07 79.52 ResNet50+B1236+MAX 40.18 79.45 ResNet50+B1236+AVE 41.07 79.97 ResNet50+B1236+MAX+DR 40.87 79.61 ResNet50+B1236+AVE+DR 41.68 80.04 Table 1. Investigation of PSPNet with different settings. Baseline is ResNet50-based FCN with dilated network. ‘B1’ and ‘B1236’ denote pooled feature maps of bin sizes {1 × 1} and {1 × 1, 2 × 2, 3 × 3, 6 × 6} respectively. ‘MAX’ and ‘AVE’ represent max pooling and average pooling operations individually. ‘DR’ means that dimension reduction is taken after pooling. The results are tested on the validation set with the single-scale input. branch [4] and make it support batch normalization on data gathered from multiple GPUs based on OpenMPI. For the auxiliary loss, we set the weight to 0.4 in experiments. 5.2. ImageNet Scene Parsing Challenge 2016 Dataset and Evaluation Metrics The ADE20K dataset [43] is used in ImageNet scene parsing challenge 2016. Dif- ferent from other datasets, ADE20K is more challenging for the up to 150 classes and diverse scenes with a total of 1,038 image-level labels. The challenge data is divided into 20K/2K/3K images for training, validation and testing. Also, it needs to parse both objects and stuff in the scene, which makes it more difﬁcult than other datasets. For eval- uation, both pixel-wise accuracy (Pixel Acc.) and mean of class-wise intersection over union (Mean IoU) are used. Ablation Study for PSPNet To evaluate PSPNet, we con- duct experiments with several settings, including pooling types of max and average, pooling with just one global fea- ture or four-level features, with and without dimension re- duction after the pooling operation and before concatena- tion. As listed in Table 1, average pooling works better than max pooling in all settings. Pooling with pyramid parsing outperforms that using global pooling. With dimension re- duction, the performance is further enhanced. With our pro- posed PSPNet, the best setting yields results 41.68/80.04 in terms of Mean IoU and Pixel Acc. (%), exceeding global average pooling of 40.07/79.52 as idea in Liu et al. [24] by 1.61/0.52. And compared to the baseline, PSPNet outper- forming it by 4.45/2.03 in terms of absolute improvement and 11.95/2.60 in terms of relative difference. Ablation Study for Auxiliary Loss The introduced aux- iliary loss helps optimize the learning process while not in- ﬂuencing learning in the master branch. We experiment with setting the auxiliary loss weight α between 0 and 1 and show the results in Table 2. The baseline uses ResNet50- based FCN with dilated network, with the master branch’s softmax loss for optimization. Adding the auxiliary loss Loss Weight α Mean IoU(%) Pixel Acc.(%) ResNet50 (without AL) 35.82 77.07 ResNet50 (with α = 0.3) 37.01 77.87 ResNet50 (with α = 0.4) 37.23 78.01 ResNet50 (with α = 0.6) 37.09 77.84 ResNet50 (with α = 0.9) 36.99 77.87 Table 2. Setting an appropriate loss weight α in the auxiliary branch is important. ‘AL’ denotes the auxiliary loss. Baseline is ResNet50-based FCN with dilated network. Empirically, α = 0.4 yields the best performance. The results are tested on the valida- tion set with the single-scale input. Figure 5. Performance grows with deeper networks. The results are obtained on the validation set with the single-scale input. Method Mean IoU(%) Pixel Acc.(%) PSPNet(50) 41.68 80.04 PSPNet(101) 41.96 80.64 PSPNet(152) 42.62 80.80 PSPNet(269) 43.81 80.88 PSPNet(50)+MS 42.78 80.76 PSPNet(101)+MS 43.29 81.39 PSPNet(152)+MS 43.51 81.38 PSPNet(269)+MS 44.94 81.69 Table 3. Deeper pre-trained model get higher performance. Num- ber in the brackets refers to the depth of ResNet and ‘MS’ denotes multi-scale testing. branch, α = 0.4 yields the best performance. It outperforms the baseline with an improvement of 1.41/0.94 in terms of Mean IoU and Pixel Acc. (%). We believe deeper networks will beneﬁt more given the new augmented auxiliary loss. Ablation Study for Pre-trained Model Deeper neural networks have been shown in previous work to be beneﬁcial to large scale data classiﬁcation. To further analyze PSPNet, we conduct experiments for different depths of pre-trained ResNet. We test four depths of {50, 101, 152, 269}. As shown in Fig. 5, with the same setting, increasing the depth of ResNet from 50 to 269 can improve the score of (Mean IoU + Pixel Acc.) / 2 (%) from 60.86 to 62.35, with 1.49 ab- solute improvement. Detailed scores of PSPNet pre-trained from different depth ResNet models are listed in Table 3. Method Mean IoU(%) Pixel Acc.(%) FCN [26] 29.39 71.32 SegNet [2] 21.64 71.00 DilatedNet [40] 32.31 73.55 CascadeNet [43] 34.90 74.52 ResNet50-Baseline 34.28 76.35 ResNet50+DA 35.82 77.07 ResNet50+DA+AL 37.23 78.01 ResNet50+DA+AL+PSP 41.68 80.04 ResNet269+DA+AL+PSP 43.81 80.88 ResNet269+DA+AL+PSP+MS 44.94 81.69 Table 4. Detailed analysis of our proposed PSPNet with compar- ison with others. Our results are obtained on the validation set with the single-scale input except for the last row. Results of FCN, SegNet and DilatedNet are reported in [43]. ‘DA’ refers to data augmentation we performed, ‘AL’ denotes the auxiliary loss we added and ‘PSP’ represents the proposed PSPNet. ‘MS’ means that multi-scale testing is used. Rank Team Name Final Score (%) 1 Ours 57.21 2 Adelaide 56.74 3 360+MCG-ICT-CAS SP 55.56 - (our single model) (55.38) 4 SegModel 54.65 5 CASIA IVA 54.33 - DilatedNet [40] 45.67 - FCN [26] 44.80 - SegNet [2] 40.79 Table 5. Results of ImageNet scene parsing challenge 2016. The best entry of each team is listed. The ﬁnal score is the mean of Mean IoU and Pixel Acc. Results are evaluated on the testing set. More Detailed Performance Analysis We show our more detailed analysis on the validation set of ADE20K in Table 4. All our results except the last-row one use single- scale test. “ResNet269+DA+AL+PSP+MS” uses multi- scale testing. Our baseline is adapted from ResNet50 with dilated network, which yields MeanIoU 34.28 and Pixel Acc. 76.35. It already outperforms other prior systems pos- sibly due to the powerful ResNet [13]. Our proposed architecture makes further improvement compared to the baseline. Using data augmentation, our result exceeds the baseline by 1.54/0.72 and reaches 35.82/77.07. Using the auxiliary loss can further improve it by 1.41/0.94 and reaches 37.23/78.01. With PSPNet, we notice relatively more signiﬁcant progress for improvement of 4.45/2.03. The result reaches 41.68/80.04. The differ- ence from the baseline result is 7.40/3.69 in terms of abso- lute improvement and 21.59/4.83 (%) in terms of relativity. A deeper network of ResNet269 yields even higher perfor- mance up to 43.81/80.88. Finally, the multi-scale testing scheme moves the scores to 44.94/81.69."
            },
            {
                "section": "Results",
                "content": "in Challenge Using the proposed architecture, our team came in the 1st place in ImageNet scene parsing Figure 6. Visual improvements on ADE20K, PSPNet produces more accurate and detailed results. challenge 2016. Table 5 shows a few results in this com- petition. Our ensemble submission achieves score 57.21% on the testing set. Our single-model yields score 55.38%, which is even higher than a few other multi-model ensem- ble submissions. This score is lower than that on the valida- tion set possibly due to the difference of data distributions between validation and testing sets. As shown in column (d) of Fig. 2, PSPNet solves the common problems in FCN. Fig. 6 shows another few parsing results on validation set of ADE20K. Our results contain more accurate and detailed structures compared to the baseline. 5.3. PASCAL VOC 2012 Our PSPNet also works satisfyingly on semantic seg- mentation. We carry out experiments on the PASCAL VOC 2012 segmentation dataset [8], which contains 20 object categories and one background class. Following the proce- dure of [26, 7, 31, 3], we use augmented data with the anno- tation of [10] resulting 10,582, 1,449 and 1,456 images for training, validation and testing. Results are shown in Ta- ble 6, we compare PSPNet with previous best-performing"
            },
            {
                "section": "methods",
                "content": "on the testing set based on two settings, i.e., with or without pre-training on MS-COCO dataset [21]. Meth- ods pre-trained with MS-COCO are marked by ‘†’. For fair comparison with current ResNet based frameworks [38, 9, 4] in scene parsing/semantic segmentation task, we build our architecture based on ResNet101 while without post- processing like CRF. We evaluate PSPNet with several- scale input and use the average results following [3, 24]. Figure 7. Visual improvements on PASCAL VOC 2012 data. PSP- Net produces more accurate and detailed results. As shown in Table 6, PSPNet outperforms prior meth- ods on both settings. Trained with only VOC 2012 data, we achieve 82.6% accuracy2 – we get the highest accuracy on all 20 classes. When PSPNet is pre-trained with MS-COCO dataset, it reaches 85.4% accuracy3 where 19 out of the 20 classes receive the highest accuracy. Intriguingly, our PSP- Net trained with only VOC 2012 data outperforms existing"
            },
            {
                "section": "methods",
                "content": "trained with the MS-COCO pre-trained model. One may argue that our based classiﬁcation model is more powerful than several prior methods since ResNet was recently proposed. To exhibit our unique contribu- tion, we show that our method also outperforms state- of-the-art frameworks that use the same model, including FCRNs [38], LRR [9], and DeepLab [4]. In this process, we even do not employ time-consuming but effective post- processing, such as CRF, as that in [4, 9]. Several examples are shown in Fig. 7. For “cows” in row one, our baseline model treats it as “horse” and “dog” while PSPNet corrects these errors. For “aeroplane” and “table” in the second and third rows, PSPNet ﬁnds missing parts. For “person”, “bottle” and “plant” in following rows, PSP- Net performs well on these small-size-object classes in the images compared to the baseline model. More visual com- parisons between PSPNet and other methods are included in Fig. 9. 5.4. Cityscapes Cityscapes [6] is a recently released dataset for semantic urban scene understanding. It contains 5,000 high quality pixel-level ﬁnely annotated images collected from 50 cities 2http://host.robots.ox.ac.uk:8080/anonymous/0OOWLP.html 3http://host.robots.ox.ac.uk:8080/anonymous/6KIR41.html Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mIoU FCN [26] 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2 Zoom-out [28] 85.6 37.3 83.2 62.5 66.0 85.1 80.7 84.9 27.2 73.2 57.5 78.1 79.2 81.1 77.1 53.6 74.0 49.2 71.7 63.3 69.6 DeepLab [3] 84.4 54.5 81.5 63.6 65.9 85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1 83.2 80.8 59.7 82.2 50.4 73.1 63.7 71.6 CRF-RNN [41] 87.5 39.0 79.7 64.2 68.3 87.6 80.8 84.4 30.4 78.2 60.4 80.5 77.8 83.1 80.6 59.5 82.8 47.8 78.3 67.1 72.0 DeconvNet [30] 89.9 39.3 79.7 63.9 68.2 87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3 83.6 80.2 58.8 83.4 54.3 80.7 65.0 72.5 GCRF [36] 85.2 43.9 83.3 65.2 68.3 89.0 82.7 85.3 31.1 79.5 63.3 80.5 79.3 85.5 81.0 60.5 85.5 52.0 77.3 65.1 73.2 DPN [25] 87.7 59.4 78.4 64.9 70.3 89.3 83.5 86.1 31.7 79.9 62.6 81.9 80.0 83.5 82.3 60.5 83.2 53.4 77.9 65.0 74.1 Piecewise [20] 90.6 37.6 80.0 67.8 74.4 92.0 85.2 86.2 39.1 81.2 58.9 83.8 83.9 84.3 84.8 62.1 83.2 58.2 80.8 72.3 75.3 PSPNet 91.8 71.9 94.7 71.2 75.8 95.2 89.9 95.9 39.3 90.7 71.7 90.5 94.5 88.8 89.6 72.8 89.6 64.0 85.1 76.3 82.6 CRF-RNN† [41] 90.4 55.3 88.7 68.4 69.8 88.3 82.4 85.1 32.6 78.5 64.4 79.6 81.9 86.4 81.8 58.6 82.4 53.5 77.4 70.1 74.7 BoxSup† [7] 89.8 38.0 89.2 68.9 68.0 89.6 83.0 87.7 34.4 83.6 67.1 81.5 83.7 85.2 83.5 58.6 84.9 55.8 81.2 70.7 75.2 Dilation8† [40] 91.7 39.6 87.8 63.1 71.8 89.7 82.9 89.8 37.2 84.0 63.0 83.3 89.0 83.8 85.1 56.8 87.6 56.0 80.2 64.7 75.3 DPN† [25] 89.0 61.6 87.7 66.8 74.7 91.2 84.3 87.6 36.5 86.3 66.1 84.4 87.8 85.6 85.4 63.6 87.3 61.3 79.4 66.4 77.5 Piecewise† [20] 94.1 40.7 84.1 67.8 75.9 93.4 84.3 88.4 42.5 86.4 64.7 85.4 89.0 85.8 86.0 67.5 90.2 63.8 80.9 73.0 78.0 FCRNs† [38] 91.9 48.1 93.4 69.3 75.5 94.2 87.5 92.8 36.7 86.9 65.2 89.1 90.2 86.5 87.2 64.6 90.1 59.7 85.5 72.7 79.1 LRR† [9] 92.4 45.1 94.6 65.2 75.8 95.1 89.1 92.3 39.0 85.7 70.4 88.6 89.4 88.6 86.6 65.8 86.2 57.4 85.7 77.3 79.3 DeepLab† [4] 92.6 60.4 91.6 63.4 76.3 95.0 88.4 92.6 32.7 88.5 67.6 89.6 92.1 87.0 87.4 63.3 88.3 60.0 86.8 74.5 79.7 PSPNet† 95.8 72.7 95.0 78.9 84.4 94.7 92.0 95.7 43.1 91.0 80.3 91.3 96.3 92.3 90.1 71.5 94.4 66.9 88.8 82.0 85.4 Table 6. Per-class results on PASCAL VOC 2012 testing set. Methods pre-trained on MS-COCO are marked with ‘†’. Method IoU cla. iIoU cla. IoU cat. iIoU cat. CRF-RNN [41] 62.5 34.4 82.7 66.0 FCN [26] 65.3 41.7 85.7 70.1 SiCNN [16] 66.3 44.9 85.0 71.2 DPN [25] 66.8 39.1 86.0 69.1 Dilation10 [40] 67.1 42.0 86.5 71.1 LRR [9] 69.7 48.0 88.2 74.7 DeepLab [4] 70.4 42.6 86.4 67.7 Piecewise [20] 71.6 51.7 87.3 74.1 PSPNet 78.4 56.7 90.6 78.6 LRR‡ [9] 71.8 47.9 88.4 73.9 PSPNet‡ 80.2 58.1 90.6 78.2 Table 7. Results on Cityscapes testing set. Methods trained using both ﬁne and coarse data are marked with ‘‡’. in different seasons. The images are divided into sets with numbers 2,975, 500, and 1,525 for training, validation and testing. It deﬁnes 19 categories containing both stuff and objects. Also, 20,000 coarsely annotated images are pro- vided for two settings in comparison, i.e., training with only ﬁne data or with both the ﬁne and coarse data. Methods trained using both ﬁne and coarse data are marked with ‘‡’. Detailed results are listed in Table 7. Our base model is ResNet101 as in DeepLab [4] for fair comparison and the testing procedure follows Section 5.3. Statistics in Table 7 show that PSPNet outperforms other"
            },
            {
                "section": "methods",
                "content": "with notable advantage. Using both ﬁne and coarse data for training makes our method yield 80.2 accuracy. Several examples are shown in Fig. 8. Detailed per-class"
            },
            {
                "section": "results",
                "content": "on testing set are shown in Table 8. 6. Concluding Remarks We have proposed an effective pyramid scene parsing network for complex scene understanding. The global pyra- Figure 8. Examples of PSPNet results on Cityscapes dataset. mid pooling feature provides additional contextual informa- tion. We have also provided a deeply supervised optimiza- tion strategy for ResNet-based FCN network. We hope the implementation details publicly available can help the com- munity adopt these useful strategies for scene parsing and semantic segmentation and advance related techniques. Acknowledgements We would like to thank Gang Sun and Tong Xiao for their help in training the basic classiﬁcation models, Qun Luo for technical support. This work is supported by a grant from the Research Grants Council of the Hong Kong SAR (project No. 2150760). Figure 9. Visual comparison on PASCAL VOC 2012 data. (a) Image. (b) Ground Truth. (c) FCN [26]. (d) DPN [24]. (e) DeepLab [4]. (f) PSPNet. Method road swalk build. wall fence pole tlight sign veg. terrain sky person rider car truck bus train mbike bike mIoU CRF-RNN [41] 96.3 73.9 88.2 47.6 41.3 35.2 49.5 59.7 90.6 66.1 93.5 70.4 34.7 90.1 39.2 57.5 55.4 43.9 54.6 62.5 FCN [26] 97.4 78.4 89.2 34.9 44.2 47.4 60.1 65.0 91.4 69.3 93.9 77.1 51.4 92.6 35.3 48.6 46.5 51.6 66.8 65.3 SiCNN+CRF [16] 96.3 76.8 88.8 40.0 45.4 50.1 63.3 69.6 90.6 67.1 92.2 77.6 55.9 90.1 39.2 51.3 44.4 54.4 66.1 66.3 DPN [25] 97.5 78.5 89.5 40.4 45.9 51.1 56.8 65.3 91.5 69.4 94.5 77.5 54.2 92.5 44.5 53.4 49.9 52.1 64.8 66.8 Dilation10 [40] 97.6 79.2 89.9 37.3 47.6 53.2 58.6 65.2 91.8 69.4 93.7 78.9 55.0 93.3 45.5 53.4 47.7 52.2 66.0 67.1 LRR [9] 97.7 79.9 90.7 44.4 48.6 58.6 68.2 72.0 92.5 69.3 94.7 81.6 60.0 94.0 43.6 56.8 47.2 54.8 69.7 69.7 DeepLab [4] 97.9 81.3 90.3 48.8 47.4 49.6 57.9 67.3 91.9 69.4 94.2 79.8 59.8 93.7 56.5 67.5 57.5 57.7 68.8 70.4 Piecewise [20] 98.0 82.6 90.6 44.0 50.7 51.1 65.0 71.7 92.0 72.0 94.1 81.5 61.1 94.3 61.1 65.1 53.8 61.6 70.6 71.6 PSPNet 98.6 86.2 92.9 50.8 58.8 64.0 75.6 79.0 93.4 72.3 95.4 86.5 71.3 95.9 68.2 79.5 73.8 69.5 77.2 78.4 LRR‡ [9] 97.9 81.5 91.4 50.5 52.7 59.4 66.8 72.7 92.5 70.1 95.0 81.3 60.1 94.3 51.2 67.7 54.6 55.6 69.6 71.8 PSPNet‡ 98.6 86.6 93.2 58.1 63.0 64.5 75.2 79.2 93.4 72.1 95.1 86.3 71.4 96.0 73.5 90.4 80.3 69.9 76.9 80.2 Table 8. Per-class results on Cityscapes testing set. Methods trained using both ﬁne and coarse set are marked with ‘‡’."
            }
        ]
    }
}